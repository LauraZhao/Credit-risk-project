{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import a bunch of stuff thats used later\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "from sklearn.ensemble.bagging import BaggingClassifier\n",
    "from sklearn.ensemble.forest import ExtraTreesClassifier\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "creditData = pd.read_csv('Desktop\\School\\Predictive Analytics w Python\\Credit Risk Project\\heloc_dataset_v1.csv', na_filter = True, keep_default_na = False)\n",
    "copy = creditData.copy() # Copy Data Just in Case, i guess?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PreProcessing Functions\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "def binarize_labels(df): #function to binarize the y-labels\n",
    "    rsk_cat = df.RiskPerformance\n",
    "    rsk_cat = rsk_cat.values.reshape(-1,1)\n",
    "    ordinal_encoder = OrdinalEncoder()\n",
    "    rsk_cat_enc = ordinal_encoder.fit_transform(rsk_cat)\n",
    "    df.RiskPerformance = rsk_cat_enc\n",
    "    return df\n",
    "def NA_fixer(df):\n",
    "    cols = df.columns #get column names\n",
    "\n",
    "    df_with_NAs = df[df > -7] #compute values less than -6 as Nas\n",
    "    df_with_NAs = df_with_NAs.dropna(how = 'all') #drop the rows that contain only missing data\n",
    "\n",
    "    cols_to_impute = []\n",
    "    for i in range(len(cols)):\n",
    "        if (len(df_with_NAs[cols[i]].dropna()) / len(df_with_NAs)) > 0.9:\n",
    "            cols_to_impute.append(cols[i]) # Collect columns where NAs make up 10% or less of the data\n",
    "\n",
    "    drop_cols = set(cols) - set(cols_to_impute)    \n",
    "    df = df_with_NAs.drop(drop_cols, axis = 1) # Drop Columns where NAs make up more than 10% of the data (5 of them)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre Process Raw Data\n",
    "copy = binarize_labels(copy)\n",
    "copy = NA_fixer(copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Split to X,y\n",
    "X = copy.drop('RiskPerformance', axis = 1) #Create X Variable by dropping y\n",
    "\n",
    "y = copy.RiskPerformance #Create y variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get numeric attributes and categoric attributes\n",
    "\n",
    "cat_cols = ['MaxDelq2PublicRecLast12M', 'MaxDelqEver']\n",
    "\n",
    "int_df = X.drop(cat_cols, axis = 1)\n",
    "num_cols = int_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Split into Training and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "#IF WE NEED 3 DATAS, UNCOMMENT THIS and COMMENT OUT THE THE ABOVE SPLIT (X_valid will be the data we use in the interface)\n",
    "# X_modeling, X_valid, y_modeling, y_valid = train_test_split(X,y, test_size = 0.01, random_state = 1)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_modeling,y_modeling, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Pipeline to Do data transformation (below cells) all at once\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "\n",
    "def pipe(df, num_cols, cat_cols):\n",
    "    \n",
    "    num_pipeline = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "            ('std_scaler', StandardScaler()),\n",
    "        ])\n",
    "    cat_pipeline = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "            ('OH_enc', OneHotEncoder()),\n",
    "        ])\n",
    "\n",
    "    full_pipeline = ColumnTransformer([\n",
    "            (\"num\", num_pipeline, num_cols),\n",
    "            (\"cat\", cat_pipeline, cat_cols),\n",
    "        ])\n",
    "\n",
    "    return pd.DataFrame(full_pipeline.fit_transform(df), index = df.index)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform Data with Pipeline\n",
    "X_train_complete = pipe(X_train, num_cols, cat_cols)\n",
    "X_test_complete = pipe(X_test, num_cols, cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-22721a70cfb0>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-11-22721a70cfb0>\"\u001b[1;36m, line \u001b[1;32m11\u001b[0m\n\u001b[1;33m    best_SVC =\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "best_gboost = GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
    "                           learning_rate=0.15000000000000002, loss='deviance',\n",
    "                           max_depth=3, max_features=None, max_leaf_nodes=None,\n",
    "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                           min_samples_leaf=1, min_samples_split=2,\n",
    "                           min_weight_fraction_leaf=0.0, n_estimators=150,\n",
    "                           n_iter_no_change=None, presort='auto',\n",
    "                           random_state=None, subsample=1.0, tol=0.0001,\n",
    "                           validation_fraction=0.1, verbose=0,\n",
    "                           warm_start=False)\n",
    "best_SVC = SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
    "                decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
    "                max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "                tol=0.001, verbose=False)\n",
    "best_Logit = LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
    "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
    "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
    "                   warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Pipelines\n",
    "Boost_Pipeline = Pipeline([\n",
    "        ('data_cleaner', pipe),\n",
    "        ('model', best_gboost)\n",
    "    ])\n",
    "SVC_Pipeline = Pipeline([\n",
    "        ('data_cleaner', pipe),\n",
    "        ('model', best_SVC)\n",
    "    ])\n",
    "Logit_Pipeline = Pipeline([\n",
    "        ('data_cleaner', pipe),\n",
    "        ('model', best_Logit)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Model\n",
    "\n",
    "def class_model_test(model):\n",
    "    model.fit(X_train_complete,y_train)\n",
    "    y_pred = model.predict(X_test_complete)\n",
    "    #mse = mean_squared_error(y_test, y_pred)\n",
    "    #rmse = np.sqrt(mse)\n",
    "    #return rmse\n",
    "    prfs =  precision_recall_fscore_support(y_test, y_pred, average = 'micro')\n",
    "    df = pd.DataFrame(list(precision_recall_fscore_support(y_test, y_pred, average = 'micro')), index = ['Precision', 'Recall', 'F Score', 'Support'])\n",
    "    return df\n",
    "    #return prfs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model SVC tested\n",
      "model Logit tested\n",
      "model Decision Tree tested\n",
      "model Random Forest tested\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isaac\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model MLP tested\n",
      "model Decision Tree Classifier tested\n",
      "model KNN tested\n",
      "model Ada Boost tested\n",
      "model Gaussian NB tested\n",
      "model Quadratic Discrimanant Analysis tested\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isaac\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:693: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model Gradient Boosting tested\n",
      "model Bagging tested\n",
      "model Extra Trees tested\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>F Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.715105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logit</td>\n",
       "      <td>0.707935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.624283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.689293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLP</td>\n",
       "      <td>0.708891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.623327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.676386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ada Boost</td>\n",
       "      <td>0.710325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Gaussian NB</td>\n",
       "      <td>0.636711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Quadratic Discrimanant Analysis</td>\n",
       "      <td>0.618547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.715583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Bagging</td>\n",
       "      <td>0.673518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Extra Trees</td>\n",
       "      <td>0.686902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Model   F Score\n",
       "0                               SVC  0.715105\n",
       "1                             Logit  0.707935\n",
       "2                     Decision Tree  0.624283\n",
       "3                     Random Forest  0.689293\n",
       "4                               MLP  0.708891\n",
       "5          Decision Tree Classifier  0.623327\n",
       "6                               KNN  0.676386\n",
       "7                         Ada Boost  0.710325\n",
       "8                       Gaussian NB  0.636711\n",
       "9   Quadratic Discrimanant Analysis  0.618547\n",
       "10                Gradient Boosting  0.715583\n",
       "11                          Bagging  0.673518\n",
       "12                      Extra Trees  0.686902"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_names = ['SVC', 'Logit', 'Decision Tree', 'Random Forest', 'MLP',\n",
    "               'Decision Tree Classifier','KNN','Ada Boost', 'Gaussian NB', 'Quadratic Discrimanant Analysis',\n",
    "               'Gradient Boosting', 'Bagging', 'Extra Trees']\n",
    "models = [SVC(), LogisticRegression(), DecisionTreeClassifier(), RandomForestClassifier(), MLPClassifier(), DecisionTreeClassifier(), KNeighborsClassifier(), \n",
    "          AdaBoostClassifier(), GaussianNB(), QuadraticDiscriminantAnalysis(), GradientBoostingClassifier(),\n",
    "          BaggingClassifier(), ExtraTreesClassifier()]\n",
    "\n",
    "\n",
    "result_list = []\n",
    "for i in range(len(models)):\n",
    "    result = class_model_test(models[i])\n",
    "    print('model ' + model_names[i] + ' tested')\n",
    "    result_list.append([model_names[i], result[0][2]])\n",
    "    model_test_results = pd.DataFrame(result_list, columns=['Model', 'F Score'])\n",
    "#model_test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper Tune Models That Perform the Best\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# SVC = SVC()\n",
    "# params = {'C': [1,5], 'gamma': [0.5, 0.1, .001], 'kernel': ['rbf','poly']} #'degree': [2,3,4]}\n",
    "\n",
    "# logit = LogisticRegression()\n",
    "# params = {'C': [0.1,0.5,1,2]}\n",
    "\n",
    "grid_search = GridSearchCV(SVC, params, cv = 5)\n",
    "grid_search.fit(X_train_complete, y_train)\n",
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "best_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
